{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5960761-22f7-4ba6-a7ca-340d43bb68d5",
   "metadata": {},
   "source": [
    "# 딥러닝으로 회귀하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2113df9c-cca0-4a7f-b537-faec77d25b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15905607\n",
      "Iteration 2, loss = 0.32990300\n",
      "Iteration 3, loss = 0.26070940\n",
      "Iteration 4, loss = 0.22816205\n",
      "Iteration 5, loss = 0.21083194\n",
      "Iteration 6, loss = 0.20145919\n",
      "Iteration 7, loss = 0.19417870\n",
      "Iteration 8, loss = 0.18773001\n",
      "Iteration 9, loss = 0.18399713\n",
      "Iteration 10, loss = 0.18033641\n",
      "Iteration 11, loss = 0.17684878\n",
      "Iteration 12, loss = 0.17478479\n",
      "Iteration 13, loss = 0.17249104\n",
      "Iteration 14, loss = 0.17020556\n",
      "Iteration 15, loss = 0.16839861\n",
      "Iteration 16, loss = 0.16629902\n",
      "Iteration 17, loss = 0.16371351\n",
      "Iteration 18, loss = 0.16180617\n",
      "Iteration 19, loss = 0.16121245\n",
      "Iteration 20, loss = 0.16013217\n",
      "Iteration 21, loss = 0.15983571\n",
      "Iteration 22, loss = 0.15652474\n",
      "Iteration 23, loss = 0.15618266\n",
      "Iteration 24, loss = 0.15367051\n",
      "Iteration 25, loss = 0.15259863\n",
      "Iteration 26, loss = 0.15360864\n",
      "Iteration 27, loss = 0.15072556\n",
      "Iteration 28, loss = 0.15254420\n",
      "Iteration 29, loss = 0.15185657\n",
      "Iteration 30, loss = 0.14795893\n",
      "Iteration 31, loss = 0.14689132\n",
      "Iteration 32, loss = 0.14700347\n",
      "Iteration 33, loss = 0.14536015\n",
      "Iteration 34, loss = 0.14484794\n",
      "Iteration 35, loss = 0.14329908\n",
      "Iteration 36, loss = 0.14298935\n",
      "Iteration 37, loss = 0.14184669\n",
      "Iteration 38, loss = 0.14144003\n",
      "Iteration 39, loss = 0.14228898\n",
      "Iteration 40, loss = 0.14120797\n",
      "Iteration 41, loss = 0.13970236\n",
      "Iteration 42, loss = 0.13991316\n",
      "Iteration 43, loss = 0.13963397\n",
      "Iteration 44, loss = 0.14021264\n",
      "Iteration 45, loss = 0.13881486\n",
      "Iteration 46, loss = 0.13872544\n",
      "Iteration 47, loss = 0.13939695\n",
      "Iteration 48, loss = 0.13779709\n",
      "Iteration 49, loss = 0.13736223\n",
      "Iteration 50, loss = 0.13638979\n",
      "Iteration 51, loss = 0.13697457\n",
      "Iteration 52, loss = 0.13554961\n",
      "Iteration 53, loss = 0.13588130\n",
      "Iteration 54, loss = 0.13479640\n",
      "Iteration 55, loss = 0.13456597\n",
      "Iteration 56, loss = 0.13534196\n",
      "Iteration 57, loss = 0.13421816\n",
      "Iteration 58, loss = 0.13444675\n",
      "Iteration 59, loss = 0.13327963\n",
      "Iteration 60, loss = 0.13310720\n",
      "Iteration 61, loss = 0.13431718\n",
      "Iteration 62, loss = 0.13263742\n",
      "Iteration 63, loss = 0.13356235\n",
      "Iteration 64, loss = 0.13390273\n",
      "Iteration 65, loss = 0.13307839\n",
      "Iteration 66, loss = 0.13155198\n",
      "Iteration 67, loss = 0.13145894\n",
      "Iteration 68, loss = 0.13132502\n",
      "Iteration 69, loss = 0.13192151\n",
      "Iteration 70, loss = 0.13146648\n",
      "Iteration 71, loss = 0.13128752\n",
      "Iteration 72, loss = 0.12956313\n",
      "Iteration 73, loss = 0.13158215\n",
      "Iteration 74, loss = 0.13053468\n",
      "Iteration 75, loss = 0.13078205\n",
      "Iteration 76, loss = 0.13195206\n",
      "Iteration 77, loss = 0.13173928\n",
      "Iteration 78, loss = 0.12970839\n",
      "Iteration 79, loss = 0.12941607\n",
      "Iteration 80, loss = 0.12952268\n",
      "Iteration 81, loss = 0.12881150\n",
      "Iteration 82, loss = 0.12948893\n",
      "Iteration 83, loss = 0.12855263\n",
      "Iteration 84, loss = 0.12782887\n",
      "Iteration 85, loss = 0.12831418\n",
      "Iteration 86, loss = 0.12717038\n",
      "Iteration 87, loss = 0.12760392\n",
      "Iteration 88, loss = 0.12722249\n",
      "Iteration 89, loss = 0.12772620\n",
      "Iteration 90, loss = 0.12653390\n",
      "Iteration 91, loss = 0.12685629\n",
      "Iteration 92, loss = 0.12698981\n",
      "Iteration 93, loss = 0.12633987\n",
      "Iteration 94, loss = 0.12683003\n",
      "Iteration 95, loss = 0.12530422\n",
      "Iteration 96, loss = 0.12646180\n",
      "Iteration 97, loss = 0.12575196\n",
      "Iteration 98, loss = 0.12548213\n",
      "Iteration 99, loss = 0.12477470\n",
      "Iteration 100, loss = 0.12471121\n",
      "Iteration 101, loss = 0.12484615\n",
      "Iteration 102, loss = 0.12367906\n",
      "Iteration 103, loss = 0.12420405\n",
      "Iteration 104, loss = 0.12436434\n",
      "Iteration 105, loss = 0.12429517\n",
      "Iteration 106, loss = 0.12441141\n",
      "Iteration 107, loss = 0.12310886\n",
      "Iteration 108, loss = 0.12430553\n",
      "Iteration 109, loss = 0.12360014\n",
      "Iteration 110, loss = 0.12358336\n",
      "Iteration 111, loss = 0.12326978\n",
      "Iteration 112, loss = 0.12391627\n",
      "Iteration 113, loss = 0.12327525\n",
      "Iteration 114, loss = 0.12246290\n",
      "Iteration 115, loss = 0.12349230\n",
      "Iteration 116, loss = 0.12490144\n",
      "Iteration 117, loss = 0.12276141\n",
      "Iteration 118, loss = 0.12260968\n",
      "Iteration 119, loss = 0.12240945\n",
      "Iteration 120, loss = 0.12157668\n",
      "Iteration 121, loss = 0.12191127\n",
      "Iteration 122, loss = 0.12136892\n",
      "Iteration 123, loss = 0.12105564\n",
      "Iteration 124, loss = 0.12159156\n",
      "Iteration 125, loss = 0.12189229\n",
      "Iteration 126, loss = 0.12151929\n",
      "Iteration 127, loss = 0.12094249\n",
      "Iteration 128, loss = 0.12184775\n",
      "Iteration 129, loss = 0.12022784\n",
      "Iteration 130, loss = 0.12133236\n",
      "Iteration 131, loss = 0.12018519\n",
      "Iteration 132, loss = 0.12056112\n",
      "Iteration 133, loss = 0.12048128\n",
      "Iteration 134, loss = 0.11914486\n",
      "Iteration 135, loss = 0.11952244\n",
      "Iteration 136, loss = 0.11995825\n",
      "Iteration 137, loss = 0.11959525\n",
      "Iteration 138, loss = 0.11878194\n",
      "Iteration 139, loss = 0.11850833\n",
      "Iteration 140, loss = 0.11880595\n",
      "Iteration 141, loss = 0.11919973\n",
      "Iteration 142, loss = 0.11796887\n",
      "Iteration 143, loss = 0.11925306\n",
      "Iteration 144, loss = 0.11899960\n",
      "Iteration 145, loss = 0.12041047\n",
      "Iteration 146, loss = 0.11855363\n",
      "Iteration 147, loss = 0.11888666\n",
      "Iteration 148, loss = 0.11940678\n",
      "Iteration 149, loss = 0.11826481\n",
      "Iteration 150, loss = 0.11891861\n",
      "Iteration 151, loss = 0.11991973\n",
      "Iteration 152, loss = 0.11808462\n",
      "Iteration 153, loss = 0.11813576\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MSE: 0.2742889195569058\n",
      "R² Score: 0.7906844930770294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링 (신경망은 스케일링 중요)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# MLP 회귀 모델 정의\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(64, 32), \n",
    "                       activation='relu', \n",
    "                       solver='adam', \n",
    "                       max_iter=500,\n",
    "                       verbose=True,\n",
    "                       random_state=42)\n",
    "\n",
    "# 학습\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = mlp_reg.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f9290-e8fc-4fef-bb1d-3fbae9d313c2",
   "metadata": {},
   "source": [
    "# 딥러닝으로 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2a4dce-81aa-48b0-b513-625500bd0afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.16455616\n",
      "Iteration 2, loss = 1.14232720\n",
      "Iteration 3, loss = 1.12082100\n",
      "Iteration 4, loss = 1.09987301\n",
      "Iteration 5, loss = 1.07952318\n",
      "Iteration 6, loss = 1.05980575\n",
      "Iteration 7, loss = 1.04066534\n",
      "Iteration 8, loss = 1.02211752\n",
      "Iteration 9, loss = 1.00404727\n",
      "Iteration 10, loss = 0.98656902\n",
      "Iteration 11, loss = 0.96965050\n",
      "Iteration 12, loss = 0.95323829\n",
      "Iteration 13, loss = 0.93724836\n",
      "Iteration 14, loss = 0.92162052\n",
      "Iteration 15, loss = 0.90638329\n",
      "Iteration 16, loss = 0.89150874\n",
      "Iteration 17, loss = 0.87692730\n",
      "Iteration 18, loss = 0.86273885\n",
      "Iteration 19, loss = 0.84883279\n",
      "Iteration 20, loss = 0.83512323\n",
      "Iteration 21, loss = 0.82159005\n",
      "Iteration 22, loss = 0.80828873\n",
      "Iteration 23, loss = 0.79516568\n",
      "Iteration 24, loss = 0.78219562\n",
      "Iteration 25, loss = 0.76938233\n",
      "Iteration 26, loss = 0.75669386\n",
      "Iteration 27, loss = 0.74413574\n",
      "Iteration 28, loss = 0.73168871\n",
      "Iteration 29, loss = 0.71943927\n",
      "Iteration 30, loss = 0.70735995\n",
      "Iteration 31, loss = 0.69548300\n",
      "Iteration 32, loss = 0.68375287\n",
      "Iteration 33, loss = 0.67217406\n",
      "Iteration 34, loss = 0.66073947\n",
      "Iteration 35, loss = 0.64947350\n",
      "Iteration 36, loss = 0.63838217\n",
      "Iteration 37, loss = 0.62743602\n",
      "Iteration 38, loss = 0.61667198\n",
      "Iteration 39, loss = 0.60607403\n",
      "Iteration 40, loss = 0.59560768\n",
      "Iteration 41, loss = 0.58522712\n",
      "Iteration 42, loss = 0.57498033\n",
      "Iteration 43, loss = 0.56489481\n",
      "Iteration 44, loss = 0.55497805\n",
      "Iteration 45, loss = 0.54521231\n",
      "Iteration 46, loss = 0.53561422\n",
      "Iteration 47, loss = 0.52620095\n",
      "Iteration 48, loss = 0.51698999\n",
      "Iteration 49, loss = 0.50801561\n",
      "Iteration 50, loss = 0.49926285\n",
      "Iteration 51, loss = 0.49070280\n",
      "Iteration 52, loss = 0.48235580\n",
      "Iteration 53, loss = 0.47419711\n",
      "Iteration 54, loss = 0.46624198\n",
      "Iteration 55, loss = 0.45847953\n",
      "Iteration 56, loss = 0.45089596\n",
      "Iteration 57, loss = 0.44351470\n",
      "Iteration 58, loss = 0.43632220\n",
      "Iteration 59, loss = 0.42929277\n",
      "Iteration 60, loss = 0.42241671\n",
      "Iteration 61, loss = 0.41569469\n",
      "Iteration 62, loss = 0.40914175\n",
      "Iteration 63, loss = 0.40275054\n",
      "Iteration 64, loss = 0.39651148\n",
      "Iteration 65, loss = 0.39042724\n",
      "Iteration 66, loss = 0.38446740\n",
      "Iteration 67, loss = 0.37863019\n",
      "Iteration 68, loss = 0.37293012\n",
      "Iteration 69, loss = 0.36735845\n",
      "Iteration 70, loss = 0.36188041\n",
      "Iteration 71, loss = 0.35650629\n",
      "Iteration 72, loss = 0.35126112\n",
      "Iteration 73, loss = 0.34611696\n",
      "Iteration 74, loss = 0.34108100\n",
      "Iteration 75, loss = 0.33613519\n",
      "Iteration 76, loss = 0.33123403\n",
      "Iteration 77, loss = 0.32640696\n",
      "Iteration 78, loss = 0.32166858\n",
      "Iteration 79, loss = 0.31701252\n",
      "Iteration 80, loss = 0.31242628\n",
      "Iteration 81, loss = 0.30796445\n",
      "Iteration 82, loss = 0.30363196\n",
      "Iteration 83, loss = 0.29942597\n",
      "Iteration 84, loss = 0.29529688\n",
      "Iteration 85, loss = 0.29122911\n",
      "Iteration 86, loss = 0.28722281\n",
      "Iteration 87, loss = 0.28326209\n",
      "Iteration 88, loss = 0.27935759\n",
      "Iteration 89, loss = 0.27551044\n",
      "Iteration 90, loss = 0.27171994\n",
      "Iteration 91, loss = 0.26798855\n",
      "Iteration 92, loss = 0.26431985\n",
      "Iteration 93, loss = 0.26069274\n",
      "Iteration 94, loss = 0.25712750\n",
      "Iteration 95, loss = 0.25362408\n",
      "Iteration 96, loss = 0.25015615\n",
      "Iteration 97, loss = 0.24672506\n",
      "Iteration 98, loss = 0.24334580\n",
      "Iteration 99, loss = 0.24001922\n",
      "Iteration 100, loss = 0.23672933\n",
      "Iteration 101, loss = 0.23348106\n",
      "Iteration 102, loss = 0.23028331\n",
      "Iteration 103, loss = 0.22713009\n",
      "Iteration 104, loss = 0.22400112\n",
      "Iteration 105, loss = 0.22087127\n",
      "Iteration 106, loss = 0.21774231\n",
      "Iteration 107, loss = 0.21464034\n",
      "Iteration 108, loss = 0.21155562\n",
      "Iteration 109, loss = 0.20849958\n",
      "Iteration 110, loss = 0.20549372\n",
      "Iteration 111, loss = 0.20254762\n",
      "Iteration 112, loss = 0.19964550\n",
      "Iteration 113, loss = 0.19678762\n",
      "Iteration 114, loss = 0.19397317\n",
      "Iteration 115, loss = 0.19120938\n",
      "Iteration 116, loss = 0.18849715\n",
      "Iteration 117, loss = 0.18583144\n",
      "Iteration 118, loss = 0.18321449\n",
      "Iteration 119, loss = 0.18066019\n",
      "Iteration 120, loss = 0.17816660\n",
      "Iteration 121, loss = 0.17573301\n",
      "Iteration 122, loss = 0.17332247\n",
      "Iteration 123, loss = 0.17094556\n",
      "Iteration 124, loss = 0.16862064\n",
      "Iteration 125, loss = 0.16634246\n",
      "Iteration 126, loss = 0.16410685\n",
      "Iteration 127, loss = 0.16191140\n",
      "Iteration 128, loss = 0.15975186\n",
      "Iteration 129, loss = 0.15762226\n",
      "Iteration 130, loss = 0.15552777\n",
      "Iteration 131, loss = 0.15349412\n",
      "Iteration 132, loss = 0.15150799\n",
      "Iteration 133, loss = 0.14956145\n",
      "Iteration 134, loss = 0.14765186\n",
      "Iteration 135, loss = 0.14578389\n",
      "Iteration 136, loss = 0.14395565\n",
      "Iteration 137, loss = 0.14216614\n",
      "Iteration 138, loss = 0.14041413\n",
      "Iteration 139, loss = 0.13869617\n",
      "Iteration 140, loss = 0.13701272\n",
      "Iteration 141, loss = 0.13536369\n",
      "Iteration 142, loss = 0.13374822\n",
      "Iteration 143, loss = 0.13216513\n",
      "Iteration 144, loss = 0.13061309\n",
      "Iteration 145, loss = 0.12909377\n",
      "Iteration 146, loss = 0.12760651\n",
      "Iteration 147, loss = 0.12615085\n",
      "Iteration 148, loss = 0.12472631\n",
      "Iteration 149, loss = 0.12333494\n",
      "Iteration 150, loss = 0.12197311\n",
      "Iteration 151, loss = 0.12063948\n",
      "Iteration 152, loss = 0.11933419\n",
      "Iteration 153, loss = 0.11805461\n",
      "Iteration 154, loss = 0.11680169\n",
      "Iteration 155, loss = 0.11557512\n",
      "Iteration 156, loss = 0.11437011\n",
      "Iteration 157, loss = 0.11319023\n",
      "Iteration 158, loss = 0.11203319\n",
      "Iteration 159, loss = 0.11089653\n",
      "Iteration 160, loss = 0.10977916\n",
      "Iteration 161, loss = 0.10868480\n",
      "Iteration 162, loss = 0.10761279\n",
      "Iteration 163, loss = 0.10656365\n",
      "Iteration 164, loss = 0.10553561\n",
      "Iteration 165, loss = 0.10452759\n",
      "Iteration 166, loss = 0.10353941\n",
      "Iteration 167, loss = 0.10257178\n",
      "Iteration 168, loss = 0.10162816\n",
      "Iteration 169, loss = 0.10070547\n",
      "Iteration 170, loss = 0.09980504\n",
      "Iteration 171, loss = 0.09892785\n",
      "Iteration 172, loss = 0.09807238\n",
      "Iteration 173, loss = 0.09723145\n",
      "Iteration 174, loss = 0.09640722\n",
      "Iteration 175, loss = 0.09558851\n",
      "Iteration 176, loss = 0.09478407\n",
      "Iteration 177, loss = 0.09399523\n",
      "Iteration 178, loss = 0.09321857\n",
      "Iteration 179, loss = 0.09245408\n",
      "Iteration 180, loss = 0.09170162\n",
      "Iteration 181, loss = 0.09096164\n",
      "Iteration 182, loss = 0.09023593\n",
      "Iteration 183, loss = 0.08952281\n",
      "Iteration 184, loss = 0.08882092\n",
      "Iteration 185, loss = 0.08813124\n",
      "Iteration 186, loss = 0.08745466\n",
      "Iteration 187, loss = 0.08678983\n",
      "Iteration 188, loss = 0.08613688\n",
      "Iteration 189, loss = 0.08549527\n",
      "Iteration 190, loss = 0.08486796\n",
      "Iteration 191, loss = 0.08425034\n",
      "Iteration 192, loss = 0.08364406\n",
      "Iteration 193, loss = 0.08304828\n",
      "Iteration 194, loss = 0.08246096\n",
      "Iteration 195, loss = 0.08188298\n",
      "Iteration 196, loss = 0.08131439\n",
      "Iteration 197, loss = 0.08075503\n",
      "Iteration 198, loss = 0.08020681\n",
      "Iteration 199, loss = 0.07966880\n",
      "Iteration 200, loss = 0.07914148\n",
      "Iteration 201, loss = 0.07863217\n",
      "Iteration 202, loss = 0.07812758\n",
      "Iteration 203, loss = 0.07762524\n",
      "Iteration 204, loss = 0.07713414\n",
      "Iteration 205, loss = 0.07665546\n",
      "Iteration 206, loss = 0.07618375\n",
      "Iteration 207, loss = 0.07571926\n",
      "Iteration 208, loss = 0.07526191\n",
      "Iteration 209, loss = 0.07481095\n",
      "Iteration 210, loss = 0.07436762\n",
      "Iteration 211, loss = 0.07393211\n",
      "Iteration 212, loss = 0.07350219\n",
      "Iteration 213, loss = 0.07307693\n",
      "Iteration 214, loss = 0.07265767\n",
      "Iteration 215, loss = 0.07225224\n",
      "Iteration 216, loss = 0.07184896\n",
      "Iteration 217, loss = 0.07146202\n",
      "Iteration 218, loss = 0.07108413\n",
      "Iteration 219, loss = 0.07071672\n",
      "Iteration 220, loss = 0.07035342\n",
      "Iteration 221, loss = 0.06999509\n",
      "Iteration 222, loss = 0.06963975\n",
      "Iteration 223, loss = 0.06928856\n",
      "Iteration 224, loss = 0.06894008\n",
      "Iteration 225, loss = 0.06859462\n",
      "Iteration 226, loss = 0.06825210\n",
      "Iteration 227, loss = 0.06791261\n",
      "Iteration 228, loss = 0.06757922\n",
      "Iteration 229, loss = 0.06725560\n",
      "Iteration 230, loss = 0.06693459\n",
      "Iteration 231, loss = 0.06661371\n",
      "Iteration 232, loss = 0.06629621\n",
      "Iteration 233, loss = 0.06598290\n",
      "Iteration 234, loss = 0.06567662\n",
      "Iteration 235, loss = 0.06537301\n",
      "Iteration 236, loss = 0.06507161\n",
      "Iteration 237, loss = 0.06477309\n",
      "Iteration 238, loss = 0.06448067\n",
      "Iteration 239, loss = 0.06419097\n",
      "Iteration 240, loss = 0.06390297\n",
      "Iteration 241, loss = 0.06362078\n",
      "Iteration 242, loss = 0.06334711\n",
      "Iteration 243, loss = 0.06307578\n",
      "Iteration 244, loss = 0.06280589\n",
      "Iteration 245, loss = 0.06254067\n",
      "Iteration 246, loss = 0.06227905\n",
      "Iteration 247, loss = 0.06201892\n",
      "Iteration 248, loss = 0.06176300\n",
      "Iteration 249, loss = 0.06151091\n",
      "Iteration 250, loss = 0.06126107\n",
      "Iteration 251, loss = 0.06101297\n",
      "Iteration 252, loss = 0.06076818\n",
      "Iteration 253, loss = 0.06052679\n",
      "Iteration 254, loss = 0.06028828\n",
      "Iteration 255, loss = 0.06005038\n",
      "Iteration 256, loss = 0.05981405\n",
      "Iteration 257, loss = 0.05957908\n",
      "Iteration 258, loss = 0.05934772\n",
      "Iteration 259, loss = 0.05911722\n",
      "Iteration 260, loss = 0.05888852\n",
      "Iteration 261, loss = 0.05865849\n",
      "Iteration 262, loss = 0.05842513\n",
      "Iteration 263, loss = 0.05818415\n",
      "Iteration 264, loss = 0.05794216\n",
      "Iteration 265, loss = 0.05770219\n",
      "Iteration 266, loss = 0.05745997\n",
      "Iteration 267, loss = 0.05722124\n",
      "Iteration 268, loss = 0.05698345\n",
      "Iteration 269, loss = 0.05674702\n",
      "Iteration 270, loss = 0.05652375\n",
      "Iteration 271, loss = 0.05629885\n",
      "Iteration 272, loss = 0.05607428\n",
      "Iteration 273, loss = 0.05584940\n",
      "Iteration 274, loss = 0.05562281\n",
      "Iteration 275, loss = 0.05539598\n",
      "Iteration 276, loss = 0.05516299\n",
      "Iteration 277, loss = 0.05492829\n",
      "Iteration 278, loss = 0.05469408\n",
      "Iteration 279, loss = 0.05445909\n",
      "Iteration 280, loss = 0.05422280\n",
      "Iteration 281, loss = 0.05397823\n",
      "Iteration 282, loss = 0.05373204\n",
      "Iteration 283, loss = 0.05348547\n",
      "Iteration 284, loss = 0.05323924\n",
      "Iteration 285, loss = 0.05300243\n",
      "Iteration 286, loss = 0.05277895\n",
      "Iteration 287, loss = 0.05255350\n",
      "Iteration 288, loss = 0.05232955\n",
      "Iteration 289, loss = 0.05210718\n",
      "Iteration 290, loss = 0.05189834\n",
      "Iteration 291, loss = 0.05169028\n",
      "Iteration 292, loss = 0.05148594\n",
      "Iteration 293, loss = 0.05128683\n",
      "Iteration 294, loss = 0.05109195\n",
      "Iteration 295, loss = 0.05089860\n",
      "Iteration 296, loss = 0.05072748\n",
      "Iteration 297, loss = 0.05058374\n",
      "Iteration 298, loss = 0.05043891\n",
      "Iteration 299, loss = 0.05029244\n",
      "Iteration 300, loss = 0.05014449\n",
      "Iteration 301, loss = 0.04999674\n",
      "Iteration 302, loss = 0.04984708\n",
      "Iteration 303, loss = 0.04969821\n",
      "Iteration 304, loss = 0.04955238\n",
      "Iteration 305, loss = 0.04940479\n",
      "Iteration 306, loss = 0.04925729\n",
      "Iteration 307, loss = 0.04911347\n",
      "Iteration 308, loss = 0.04896884\n",
      "Iteration 309, loss = 0.04882305\n",
      "Iteration 310, loss = 0.04867604\n",
      "Iteration 311, loss = 0.04852903\n",
      "Iteration 312, loss = 0.04838267\n",
      "Iteration 313, loss = 0.04823521\n",
      "Iteration 314, loss = 0.04808700\n",
      "Iteration 315, loss = 0.04794059\n",
      "Iteration 316, loss = 0.04781807\n",
      "Iteration 317, loss = 0.04769884\n",
      "Iteration 318, loss = 0.04757501\n",
      "Iteration 319, loss = 0.04744867\n",
      "Iteration 320, loss = 0.04732059\n",
      "Iteration 321, loss = 0.04719665\n",
      "Iteration 322, loss = 0.04707008\n",
      "Iteration 323, loss = 0.04694125\n",
      "Iteration 324, loss = 0.04681906\n",
      "Iteration 325, loss = 0.04670787\n",
      "Iteration 326, loss = 0.04659582\n",
      "Iteration 327, loss = 0.04648309\n",
      "Iteration 328, loss = 0.04636902\n",
      "Iteration 329, loss = 0.04625428\n",
      "Iteration 330, loss = 0.04613832\n",
      "Iteration 331, loss = 0.04602161\n",
      "Iteration 332, loss = 0.04590553\n",
      "Iteration 333, loss = 0.04579884\n",
      "Iteration 334, loss = 0.04569442\n",
      "Iteration 335, loss = 0.04558687\n",
      "Iteration 336, loss = 0.04547612\n",
      "Iteration 337, loss = 0.04536000\n",
      "Iteration 338, loss = 0.04526038\n",
      "Iteration 339, loss = 0.04516445\n",
      "Iteration 340, loss = 0.04506659\n",
      "Iteration 341, loss = 0.04496465\n",
      "Iteration 342, loss = 0.04486399\n",
      "Iteration 343, loss = 0.04476243\n",
      "Iteration 344, loss = 0.04465557\n",
      "Iteration 345, loss = 0.04455329\n",
      "Iteration 346, loss = 0.04446718\n",
      "Iteration 347, loss = 0.04437693\n",
      "Iteration 348, loss = 0.04428080\n",
      "Iteration 349, loss = 0.04418047\n",
      "Iteration 350, loss = 0.04407751\n",
      "Iteration 351, loss = 0.04399118\n",
      "Iteration 352, loss = 0.04390240\n",
      "Iteration 353, loss = 0.04381710\n",
      "Iteration 354, loss = 0.04372993\n",
      "Iteration 355, loss = 0.04363925\n",
      "Iteration 356, loss = 0.04354819\n",
      "Iteration 357, loss = 0.04346034\n",
      "Iteration 358, loss = 0.04336576\n",
      "Iteration 359, loss = 0.04327713\n",
      "Iteration 360, loss = 0.04319598\n",
      "Iteration 361, loss = 0.04310976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# MLP 분류 모델 정의\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(50, 30), \n",
    "                        activation='relu',\n",
    "                        solver='adam',\n",
    "                        max_iter=500,\n",
    "                        verbose = True,\n",
    "                        random_state=42)\n",
    "\n",
    "# 학습\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = mlp_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7aba8-5d87-4d71-97a0-f9f972140d96",
   "metadata": {},
   "source": [
    "### 더 넣어볼 수 있는 하이퍼파라미터\n",
    "\n",
    "- activattion\n",
    "    - 'relu'\n",
    "    - 'tanh'\n",
    "    - 'logistic'\n",
    "- solver\n",
    "    - 'adam' : 빠르고 안정적 (기본값)\n",
    "    - 'lbfgs' : 작은 데이터셋에서 좋음\n",
    "    - 'sgd' : 학습률 튜닝 필요, 대규모 데이터에 적합\n",
    "- learning_rate_init(학습율)\n",
    "    - 기본값 -> 0.001 (0.001~0.01 사이에서 조절하는 것을 권장, 경우에 따라 0.00001까지 가기도)\n",
    "    - 너무 크면 발산, 너무 작으면 매우 느린 수렴 (local minimum 문제도 발생)\n",
    "- alpha (L2 규제)\n",
    "    - 기본값 -> 0.0001\n",
    "    - 값이 클 수록 과적합 억제 경향이 강해짐, 근데 너무 강해지면 underfitting의 위험\n",
    "- max_iter (최대 반복 횟수)\n",
    "- learning_rate (학습율 스케줄링 방식)\n",
    "    - 'constant' : 고정된 학습율 (기본값)\n",
    "    - 'invscaling' : 점점 감소\n",
    "    - 'adaptive' : 개선이 없을 때 감소\n",
    "- early_stopping (자동 조기 종료)\n",
    "    - True로 설정하면 작동함\n",
    "- validation_fraction (validation 데이터 자동으로 분리)\n",
    "    - 0.1 이런식으로 숫자를 지정하면 그만큼 생성되서 평가를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "097931e9-e141-484a-8784-b4a97cc26a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),  # 은닉층 3개\n",
    "    activation='relu',                # 활성화 함수\n",
    "    solver='adam',                    # 옵티마이저\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479acd6-fdd8-486d-8041-883b026f2224",
   "metadata": {},
   "source": [
    "# GrdSearch를 딥러닝에 적용한다면? (회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963656f7-895d-45d0-94f4-450f33203501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab8bf0c7-7d27-4930-9503-97a988873b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Iteration 1, loss = 0.29408745\n",
      "Iteration 2, loss = 0.18258618\n",
      "Iteration 3, loss = 0.17010833\n",
      "Iteration 4, loss = 0.16400688\n",
      "Iteration 5, loss = 0.15708706\n",
      "최적 하이퍼파라미터: {'activation': 'tanh', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "CV 평균 R²: 0.7571084381992009\n",
      "테스트 R²: 0.7536055076156115\n",
      "MSE: 0.3228775550096117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sju12\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 모델 정의\n",
    "mlp = MLPRegressor(max_iter=5, verbose=True, random_state=42)\n",
    "\n",
    "# 파라미터 그리드\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64,), (128,64), (128,64,32)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(mlp, param_grid, cv=3, scoring='r2', n_jobs=-1, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"최적 하이퍼파라미터:\", grid.best_params_)\n",
    "print(\"CV 평균 R²:\", grid.best_score_)\n",
    "\n",
    "# 최적 모델 평가\n",
    "best_mlp = grid.best_estimator_\n",
    "y_pred = best_mlp.predict(X_test)\n",
    "print(\"테스트 R²:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be5c47-894f-43c1-bcbd-b6532173a22d",
   "metadata": {},
   "source": [
    "# GrdSearch를 딥러닝에 적용한다면? (분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96c61edb-82d2-4ce7-96c7-2ebf7339748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Iteration 1, loss = 1.08946168\n",
      "Iteration 2, loss = 0.44367775\n",
      "Iteration 3, loss = 0.33208374\n",
      "Iteration 4, loss = 0.27172146\n",
      "Iteration 5, loss = 0.22097492\n",
      "최적 하이퍼파라미터: {'activation': 'tanh', 'hidden_layer_sizes': (128, 64, 32), 'learning_rate_init': 0.01, 'solver': 'adam'}\n",
      "최적 CV 점수: 0.9333333333333333\n",
      "테스트 정확도: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sju12\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 분할\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# MLP 모델\n",
    "mlp = MLPClassifier(max_iter=5, verbose=True, random_state=42)\n",
    "\n",
    "# 탐색할 하이퍼파라미터 설정\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (128, 64, 32)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# GridSearchCV 구성\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,              # 5-Fold 교차검증\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,         # 병렬 처리\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "# 학습\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"최적 하이퍼파라미터:\", grid_search.best_params_)\n",
    "print(\"최적 CV 점수:\", grid_search.best_score_)\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"테스트 정확도:\", best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b696c0c-f9b4-43d3-96f0-6a057aa395e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
