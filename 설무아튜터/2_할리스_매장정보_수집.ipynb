{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í• ë¦¬ìŠ¤ ë§¤ì¥ì •ë³´ ì›¹ìŠ¤í¬ë˜í•‘ ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ì›¹ í˜ì´ì§€ ë¶„ì„í•˜ê¸°\n",
    "- ì›¹ í¬ë¡¤ë§ì„ ì‹œì‘í•˜ê¸° ì „ì— ëŒ€ìƒ ì›¹ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### 0.1 URL ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„\n",
    "- **ëŒ€ìƒ URL**: `https://www.hollys.co.kr/store/korea/korStore2.do`\n",
    "- **HTTP ë©”ì„œë“œ**: GET\n",
    "- **ì„¤ëª…**: í• ë¦¬ìŠ¤ ì»¤í”¼ ë§¤ì¥ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” í˜ì´ì§€\n",
    "\n",
    "### 0.2 ìš”ì²­ íŒŒë¼ë¯¸í„° ë¶„ì„\n",
    "- **pageNo**: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)\n",
    "  - ì˜ˆì‹œ: `?pageNo=1`, `?pageNo=2`\n",
    "  - ê° í˜ì´ì§€ë§ˆë‹¤ ì—¬ëŸ¬ ë§¤ì¥ ì •ë³´ë¥¼ í¬í•¨\n",
    "\n",
    "### 0.3 HTML êµ¬ì¡° ë¶„ì„ (ê°œë°œì ë„êµ¬ í™œìš©)\n",
    "\n",
    "#### ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ ì‚¬ìš©ë²•:\n",
    "1. ì›¹ í˜ì´ì§€ ì ‘ì† í›„ ê°œë°œì ë„êµ¬ ì—´ê¸°\n",
    "    - **Windows**: `F12` ë˜ëŠ” `Ctrl + Shift + I` ë˜ëŠ” `ìš°í´ë¦­ > ê²€ì‚¬`\n",
    "    - **Mac**: `Cmd + Option + I` ë˜ëŠ” `ìš°í´ë¦­ > ê²€ì‚¬`\n",
    "2. Elements íƒ­ì—ì„œ ì›í•˜ëŠ” ë°ì´í„° ìœ„ì¹˜ë¥¼ ì°¾ì•„ HTML êµ¬ì¡° í™•ì¸\n",
    "\n",
    "#### ë§¤ì¥ ì •ë³´ í…Œì´ë¸” êµ¬ì¡°:\n",
    "```html\n",
    "<table class=\"tb_store\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>ì§€ì—­</th>\n",
    "            <th>ë§¤ì¥ëª…</th>\n",
    "            <th>ì˜ì—…ìƒíƒœ</th>\n",
    "            <th>ì£¼ì†Œ</th>\n",
    "            <th>ì„œë¹„ìŠ¤</th>\n",
    "            <th>ì „í™”ë²ˆí˜¸</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>ì„œìš¸</td>\n",
    "            <td>ê°•ë‚¨ì </td>\n",
    "            <td>ì˜ì—…ì¤‘</td>\n",
    "            <td>ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ ...</td>\n",
    "            <td><img alt=\"ì™€ì´íŒŒì´\" /></td>\n",
    "            <td>02-1234-5678</td>\n",
    "        </tr>\n",
    "        <!-- ë” ë§ì€ ë§¤ì¥ ì •ë³´ ... -->\n",
    "    </tbody>\n",
    "</table>\n",
    "```\n",
    "\n",
    "### 0.4 CSS ì„ íƒì ê²½ë¡œ\n",
    "- **í…Œì´ë¸” ì „ì²´**: `table.tb_store`\n",
    "- **ëª¨ë“  ë§¤ì¥ í–‰**: `table.tb_store > tbody > tr`\n",
    "- **ê°œë³„ ì…€ êµ¬ì¡°**:\n",
    "  - `td:nth-child(1)`: ì§€ì—­\n",
    "  - `td:nth-child(2)`: ë§¤ì¥ëª…\n",
    "  - `td:nth-child(3)`: ì˜ì—…ìƒíƒœ\n",
    "  - `td:nth-child(4)`: ì£¼ì†Œ\n",
    "  - `td:nth-child(5)`: ì„œë¹„ìŠ¤ (ì´ë¯¸ì§€)\n",
    "  - `td:nth-child(6)`: ì „í™”ë²ˆí˜¸\n",
    "\n",
    "### 0.5 ì„œë¹„ìŠ¤ ì•„ì´ì½˜ ë¶„ì„\n",
    "- ì„œë¹„ìŠ¤ ì •ë³´ëŠ” `<img>` íƒœê·¸ì˜ `alt` ì†ì„±ì— ì €ì¥\n",
    "- ì˜ˆì‹œ: `<img alt=\"ì™€ì´íŒŒì´\" />`, `<img alt=\"ì£¼ì°¨\" />`\n",
    "- ì—¬ëŸ¬ ì„œë¹„ìŠ¤ê°€ ìˆì„ ê²½ìš° ê³µë°±ìœ¼ë¡œ ê²°í•©\n",
    "\n",
    "### 0.6 ìš”ì²­ í—¤ë” ì„¤ì •\n",
    "- **User-Agent** ì„¤ì • í•„ìš”\n",
    "  - ì¼ë¶€ ì›¹ì‚¬ì´íŠ¸ëŠ” User-Agentê°€ ì—†ëŠ” ìš”ì²­ì„ ì°¨ë‹¨\n",
    "  - ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ë„ë¡ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ì›¹ í˜ì´ì§€ ìš”ì²­í•˜ê¸°\n",
    "- ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_aitimes_article_list(page: int, total: int = 28654) -> str:\n",
    "    \"\"\"\n",
    "    AIíƒ€ì„ì¦ˆ ë‰´ìŠ¤ AJAX í˜ì´ì§€ì˜ JSON/HTMLì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    url = \"https://www.aitimes.com/news/ajaxArticlePaging.php\"\n",
    "    params = {\n",
    "        \"total\": str(total),          # âœ… total ê°’ ì¶”ê°€\n",
    "        \"list_per_page\": \"20\",\n",
    "        \"page_per_page\": \"10\",\n",
    "        \"page\": str(page),\n",
    "        \"view_type\": \"sm\",\n",
    "        \"box_idxno\": \"0\"\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36',\n",
    "        'Referer': f'https://www.aitimes.com/news/articleList.html?page={page}',\n",
    "        'Accept': 'application/json, text/javascript, */*; q=0.01',  # âœ… JSON ì‘ë‹µ ê³ ë ¤\n",
    "        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'X-Requested-With': 'XMLHttpRequest',  # âœ… AJAX ìš”ì²­ì„ì„ ëª…ì‹œ\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text  # JSON ë¬¸ìì—´ ë°˜í™˜\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"í˜ì´ì§€ {page} ìš”ì²­ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\":\"success\",\"msg\":\"\",\"data\":[{\"idxno\":\"203546\",\"title\":\"\\uacfc\\uae30\\ubd80, APEC \\ud55c-\\ubbf8 \\uc815\\uc0c1\\ud68c\\ub2f4\\uc11c AI\\uae30\\uc220 \\ud611\\ub825 MOU \\uccb4\\uacb0\",\"shoulder_title\":\"\",\"sub_title\":\"\",\"body\":\"(\\uc0ac\\uc9c4=\\uacfc\\uae30\\ubd80)        \\uacfc\\ud559\\uae30\\uc220\\uc815\\ubcf4\\ud1b5\\uc2e0\\ubd80(\\uc7a5\\uad00 \\ubc30\\uacbd\\ud6c8)\\ub294 APEC \\uc8fc\\uac04\\uc5d0 \\uc5f4\\ub9b0 \\ud55c-\\ubbf8 \\uc815\\uc0c1\\ud68c\\ub2f4\\uc758 \\uc131\\uacfc\\ub85c \\uc591\\uad6d \\uc815\\ubd80 \\uac04 MOU\\ub97c \\uccb4\\uacb0\\ud588\\ub2e4\\uace0 29\\uc77c \\ubc1d\\ud614\\ub2e4.    \\ucd5c\\uadfc \\uc778\\uacf5\\uc9c0\\ub2a5(AI), \\uc591\\uc790 \\ucef4\\ud4e8\\ud305, \\ud569\\uc131\\uc0dd\\ubb3c\\ud559 \\ub4f1 \\ud575\\uc2ec \\uc2e0\\ud765 \\uae30\\uc220\\uc758 \\uc911\\uc694\\uc131\\uc774 \\ub0a0\\ub85c \\ucee4\\uc9d0\\uc5d0 \\ub530\\ub77c, \\uc804\\ubc29\\uc704\\uc801 \\uae30\\uc220 \\ud611\\ub825\\uccb4\\uacc4\\ub97c \\uac15\\ud654\\ud558\\uae30 \\uc704\\ud574 \\uc774\\ubc88 MOU\\ub97c \\uccb4\\uacb0\\ud588\\ub2e4\\ub294 \\uc124\\uba85\\uc774\\ub2e4.    \\uc591\\uad6d\\uc\n"
     ]
    }
   ],
   "source": [
    "html = get_aitimes_article_list(1)\n",
    "print(html[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HTML íŒŒì‹±í•˜ê¸°\n",
    "- BeautifulSoupë¥¼ ì‚¬ìš©í•˜ì—¬ HTMLì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_store_info(html: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    HTMLì—ì„œ ë§¤ì¥ ì •ë³´ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        html (str): íŒŒì‹±í•  HTML ë¬¸ìì—´\n",
    "    Returns:\n",
    "        List[Dict]: ë§¤ì¥ ì •ë³´ ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    stores = []\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # CSS ì„ íƒìë¡œ table.tb_store > tbody > tr ì„ íƒ\n",
    "    rows = soup.select(\"article.altlist-body > ul > li\")\n",
    "    \n",
    "    # find() í•¨ìˆ˜ ì´ìš© ë‹¨ì¼ íƒœê·¸ë¡œ ì°¾ê¸°\n",
    "    # table = soup.find(\"table\", class_=\"tb_store\")\n",
    "    # tbody = table.find(\"tbody\")\n",
    "    # rows = tbody.find_all(\"tr\")\n",
    "   \n",
    "    for tr in rows:\n",
    "        tds = tr.find_all('div')\n",
    "        if len(tds) < 6:\n",
    "            continue\n",
    "        \n",
    "        store = {\n",
    "            'region': tds[0].text.strip(),\n",
    "            'name': tds[1].text.strip(),\n",
    "            'status': tds[2].text.strip(),\n",
    "            'address': tds[3].text.strip(),\n",
    "            'service': ' '.join(img['alt'] for img in tds[4].find_all('img')),\n",
    "            'phone': tds[5].text.strip()\n",
    "        }\n",
    "        stores.append(store)\n",
    "    \n",
    "    return stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸: ì²« í˜ì´ì§€ íŒŒì‹±í•˜ê¸°\n",
    "stores = parse_store_info(html)\n",
    "print(f\"ì²« í˜ì´ì§€ì—ì„œ ì°¾ì€ ë§¤ì¥ ìˆ˜: {len(stores)}\")\n",
    "print(\"\\nì²« ë²ˆì§¸ ë§¤ì¥ ì •ë³´:\")\n",
    "pprint(stores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì €ì¥í•˜ê¸°\n",
    "- ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ CSVì™€ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_files(stores: List[Dict], base_path: str = \"./ì›¹í¬ë¡¤ë§/ì»¤í”¼ë§¤ì¥\"):\n",
    "   \"\"\"\n",
    "   ë§¤ì¥ ì •ë³´ë¥¼ CSVì™€ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "   \"\"\"\n",
    "   \n",
    "   # ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±)\n",
    "   os.makedirs(base_path, exist_ok=True)\n",
    "   \n",
    "   # DataFrame ìƒì„±\n",
    "   df = pd.DataFrame(stores)\n",
    "   \n",
    "   # CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "   file_name = \"hollys_stores.csv\"\n",
    "   csv_file_path = os.path.join(base_path, file_name)\n",
    "   df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "   \n",
    "   # JSON íŒŒì¼ë¡œ ì €ì¥ (DataFrame í™œìš©)\n",
    "   file_name = f\"hollys_stores.json\"\n",
    "   json_file_path = os.path.join(base_path, file_name)\n",
    "   df.to_json(json_file_path, orient='records', force_ascii=False, indent=4)\n",
    "   \n",
    "   print(f\"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_file_path}\")\n",
    "   print(f\"JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {json_file_path}\")\n",
    "   \n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸: ì²« í˜ì´ì§€ ë°ì´í„° ì €ì¥í•˜ê¸°\n",
    "df = save_to_files(stores)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì „ì²´ ê³¼ì • ì‹¤í–‰í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = []\n",
    "\n",
    "# ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘\n",
    "print(\"í• ë¦¬ìŠ¤ ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘ ì¤‘...\")\n",
    "for page in range(1, 4):  # 1~3 í˜ì´ì§€ë§Œ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "    html = get_hollys_store_info(page)\n",
    "    if html:\n",
    "        page_stores = parse_store_info(html)\n",
    "        stores.extend(page_stores)\n",
    "        print(f\"í˜ì´ì§€ {page}: {len(page_stores)}ê°œì˜ ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "\n",
    "if not stores:\n",
    "    print(\"ë§¤ì¥ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "print(f\"\\nì´ {len(stores)}ê°œì˜ ë§¤ì¥ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ë°ì´í„° ì €ì¥\n",
    "try:\n",
    "    save_to_files(stores)\n",
    "except Exception as e:\n",
    "    print(f\"ë°ì´í„° ì €ì¥ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ 1í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\n",
      "ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (): Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
      "ğŸ“„ 2í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\n",
      "ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (): Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
      "âœ… aitimes_ai_news.json ì €ì¥ ì™„ë£Œ (42ê°œ ê¸°ì‚¬)\n",
      "\n",
      "ğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\n",
      "date\n",
      "               2\n",
      "10-27 18:00    1\n",
      "10-27 16:40    1\n",
      "10-27 13:05    1\n",
      "10-24 17:00    1\n",
      "10-24 15:52    1\n",
      "10-23 16:54    1\n",
      "10-22 12:44    1\n",
      "10-29 15:45    1\n",
      "10-02 14:09    1\n",
      "10-01 06:00    1\n",
      "09-26 10:05    1\n",
      "09-30 18:11    1\n",
      "09-16 18:50    1\n",
      "09-15 16:50    1\n",
      "09-15 16:14    1\n",
      "09-18 18:30    1\n",
      "09-13 14:50    1\n",
      "09-13 10:20    1\n",
      "09-11 15:40    1\n",
      "09-10 16:05    1\n",
      "09-09 17:00    1\n",
      "09-09 12:27    1\n",
      "09-08 16:30    1\n",
      "09-08 16:05    1\n",
      "09-05 16:00    1\n",
      "09-04 14:50    1\n",
      "09-03 16:15    1\n",
      "09-01 14:50    1\n",
      "08-29 17:05    1\n",
      "08-29 16:35    1\n",
      "08-29 12:20    1\n",
      "08-28 11:41    1\n",
      "08-25 10:50    1\n",
      "08-24 16:25    1\n",
      "08-22 16:58    1\n",
      "08-22 16:00    1\n",
      "08-20 07:35    1\n",
      "08-19 16:30    1\n",
      "08-17 16:45    1\n",
      "08-15 07:26    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“° ìµœì‹  ê¸°ì‚¬:\n",
      "                                              title         date reporter\n",
      "0                ê³¼ê¸°ë¶€, APEC í•œ-ë¯¸ ì •ìƒíšŒë‹´ì„œ AIê¸°ìˆ  í˜‘ë ¥ MOU ì²´ê²°  10-29 15:45   ì¥ì„¸ë¯¼ ê¸°ì\n",
      "1  APEC CEO ì„œë°‹ì— ê¸€ë¡œë²Œ ê¸°ì—…ì¸ 1700ëª… ì°¸ì—¬...\"K-ì‚°ì—… ë¯¸ë˜ ì²­ì‚¬ì§„ ì œì‹œ\"  10-27 18:00   ì¥ì„¸ë¯¼ ê¸°ì\n",
      "2         ê³¼ê¸°ë¶€, 'AI ë…¸ì½”ë“œ í•´ì»¤í†¤' ê°œìµœ...\"20ì„¸ ì´ìƒ ëˆ„êµ¬ë‚˜ ì°¸ì—¬ ê°€ëŠ¥\"  10-27 16:40   ì¥ì„¸ë¯¼ ê¸°ì\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "def get_article_list(page: int = 1, section_code: str = \"S1N1\") -> list:\n",
    "    \"\"\"\n",
    "    AIíƒ€ì„ì¦ˆ ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.aitimes.com/news/articleList.html\"\n",
    "    params = {\n",
    "        \"page\": str(page),\n",
    "        \"sc_section_code\": section_code,  # AI ê´€ë ¨ ì„¹ì…˜ ì½”ë“œ\n",
    "        \"view_type\": \"sm\"\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    # ê¸°ì‚¬ ë¸”ë¡ ì„ íƒ\n",
    "    for div in soup.select(\"div.altlist-webzine-content\"):\n",
    "        title_tag = div.select_one(\"h2.altlist-subject a\")\n",
    "        info_items = div.select(\"div.altlist-info-item\")\n",
    "\n",
    "        if not title_tag or len(info_items) < 2:\n",
    "            continue\n",
    "\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        url = title_tag[\"href\"]\n",
    "        reporter = info_items[0].get_text(strip=True)\n",
    "        date = info_items[1].get_text(strip=True)\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"reporter\": reporter,\n",
    "            \"date\": date\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_article_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    ê°œë³„ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content_div = soup.select_one(\"div.article-body\")  # ë³¸ë¬¸ ë¶€ë¶„\n",
    "        if not content_div:\n",
    "            content_div = soup.select_one(\"div#article-view-content-div\")\n",
    "\n",
    "        content = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"\"\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def crawl_ai_news(pages: int = 3):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ AI ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\"ğŸ“„ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        articles = get_article_list(page)\n",
    "\n",
    "        for article in articles:\n",
    "            article[\"content\"] = get_article_content(article[\"url\"])\n",
    "            all_articles.append(article)\n",
    "            sleep(1)  # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "\n",
    "def save_to_json(data: list, filename: str = \"aitimes_ai_news.json\"):\n",
    "    \"\"\"\n",
    "    ê²°ê³¼ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… {filename} ì €ì¥ ì™„ë£Œ ({len(data)}ê°œ ê¸°ì‚¬)\")\n",
    "\n",
    "\n",
    "def summarize_articles(data: list):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ë“± ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\")\n",
    "    print(df[\"date\"].value_counts())\n",
    "\n",
    "    print(\"\\nğŸ“° ìµœì‹  ê¸°ì‚¬:\")\n",
    "    print(df.head(3)[[\"title\", \"date\", \"reporter\"]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_ai_news(pages=2)  # ì›í•˜ëŠ” í˜ì´ì§€ ìˆ˜ ì¡°ì •\n",
    "    save_to_json(data)\n",
    "    summarize_articles(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ 1í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\n",
      "ğŸ“„ 2í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\n",
      "âœ… aitimes_ai_news.json ì €ì¥ ì™„ë£Œ (42ê°œ ê¸°ì‚¬)\n",
      "\n",
      "ğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\n",
      "date\n",
      "               2\n",
      "10-27 18:00    1\n",
      "10-27 16:40    1\n",
      "10-27 13:05    1\n",
      "10-24 17:00    1\n",
      "10-24 15:52    1\n",
      "10-23 16:54    1\n",
      "10-22 12:44    1\n",
      "10-29 15:45    1\n",
      "10-02 14:09    1\n",
      "10-01 06:00    1\n",
      "09-26 10:05    1\n",
      "09-30 18:11    1\n",
      "09-16 18:50    1\n",
      "09-15 16:50    1\n",
      "09-15 16:14    1\n",
      "09-18 18:30    1\n",
      "09-13 14:50    1\n",
      "09-13 10:20    1\n",
      "09-11 15:40    1\n",
      "09-10 16:05    1\n",
      "09-09 17:00    1\n",
      "09-09 12:27    1\n",
      "09-08 16:30    1\n",
      "09-08 16:05    1\n",
      "09-05 16:00    1\n",
      "09-04 14:50    1\n",
      "09-03 16:15    1\n",
      "09-01 14:50    1\n",
      "08-29 17:05    1\n",
      "08-29 16:35    1\n",
      "08-29 12:20    1\n",
      "08-28 11:41    1\n",
      "08-25 10:50    1\n",
      "08-24 16:25    1\n",
      "08-22 16:58    1\n",
      "08-22 16:00    1\n",
      "08-20 07:35    1\n",
      "08-19 16:30    1\n",
      "08-17 16:45    1\n",
      "08-15 07:26    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“° ìµœì‹  ê¸°ì‚¬:\n",
      "                                              title         date reporter\n",
      "0                ê³¼ê¸°ë¶€, APEC í•œ-ë¯¸ ì •ìƒíšŒë‹´ì„œ AIê¸°ìˆ  í˜‘ë ¥ MOU ì²´ê²°  10-29 15:45   ì¥ì„¸ë¯¼ ê¸°ì\n",
      "1  APEC CEO ì„œë°‹ì— ê¸€ë¡œë²Œ ê¸°ì—…ì¸ 1700ëª… ì°¸ì—¬...\"K-ì‚°ì—… ë¯¸ë˜ ì²­ì‚¬ì§„ ì œì‹œ\"  10-27 18:00   ì¥ì„¸ë¯¼ ê¸°ì\n",
      "2         ê³¼ê¸°ë¶€, 'AI ë…¸ì½”ë“œ í•´ì»¤í†¤' ê°œìµœ...\"20ì„¸ ì´ìƒ ëˆ„êµ¬ë‚˜ ì°¸ì—¬ ê°€ëŠ¥\"  10-27 16:40   ì¥ì„¸ë¯¼ ê¸°ì\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = \"https://www.aitimes.com\"\n",
    "\n",
    "def get_article_list(page: int = 1, section_code: str = \"S1N1\") -> list:\n",
    "    \"\"\"\n",
    "    AIíƒ€ì„ì¦ˆ ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.aitimes.com/news/articleList.html\"\n",
    "    params = {\n",
    "        \"page\": str(page),\n",
    "        \"sc_section_code\": section_code,\n",
    "        \"view_type\": \"sm\"\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    for div in soup.select(\"div.altlist-webzine-content\"):\n",
    "        title_tag = div.select_one(\"h2.altlist-subject a\")\n",
    "        info_items = div.select(\"div.altlist-info-item\")\n",
    "\n",
    "        if not title_tag or len(info_items) < 2:\n",
    "            continue\n",
    "\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        relative_url = title_tag[\"href\"]\n",
    "        url = urljoin(BASE_URL, relative_url)   # âœ… ì ˆëŒ€ê²½ë¡œ ë³€í™˜\n",
    "        reporter = info_items[0].get_text(strip=True)\n",
    "        date = info_items[1].get_text(strip=True)\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"reporter\": reporter,\n",
    "            \"date\": date\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_article_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    ê°œë³„ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # âœ… ë³¸ë¬¸ ì„ íƒì ìˆ˜ì •\n",
    "        paragraphs = soup.select(\"article.article-veiw-body.view-page.font-size18 p\")\n",
    "        content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def crawl_ai_news(pages: int = 3):\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\"ğŸ“„ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        articles = get_article_list(page)\n",
    "\n",
    "        for article in articles:\n",
    "            article[\"content\"] = get_article_content(article[\"url\"])\n",
    "            all_articles.append(article)\n",
    "        sleep(0.\n",
    "              1)\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "\n",
    "def save_to_json(data: list, filename: str = \"aitimes_ai_news.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… {filename} ì €ì¥ ì™„ë£Œ ({len(data)}ê°œ ê¸°ì‚¬)\")\n",
    "\n",
    "\n",
    "def summarize_articles(data: list):\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\")\n",
    "    print(df[\"date\"].value_counts())\n",
    "\n",
    "    print(\"\\nğŸ“° ìµœì‹  ê¸°ì‚¬:\")\n",
    "    print(df.head(3)[[\"title\", \"date\", \"reporter\"]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_ai_news(pages=2)\n",
    "    save_to_json(data)\n",
    "    summarize_articles(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
