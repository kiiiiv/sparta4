{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7351d7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 저장 완료: results/aitimes_news.json (170건)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 기본 설정\n",
    "# -----------------------------\n",
    "BASE_LIST = \"https://www.aitimes.com/news/articleList.html\"\n",
    "BASE_ORIGIN = \"https://www.aitimes.com\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/122.0.0.0\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 목록 페이지 HTML 가져오기\n",
    "# -----------------------------\n",
    "def get_list_html(page=1):\n",
    "    params = {\"view_type\": \"sm\", \"page\": page}\n",
    "    try:\n",
    "        r = requests.get(BASE_LIST, headers=HEADERS, params=params, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] 페이지 {page} 요청 실패: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# -----------------------------\n",
    "# 기사 목록 파싱\n",
    "# -----------------------------\n",
    "def parse_list(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    for a in soup.select('a[href*=\"/news/articleView.html\"]'):\n",
    "        url = BASE_ORIGIN + a.get(\"href\")\n",
    "        title = a.get_text(strip=True)\n",
    "        if not title:\n",
    "            continue\n",
    "\n",
    "        # 요약, 날짜 선택자 시도\n",
    "        summary_node = a.find_next(\"p\")\n",
    "        summary = summary_node.get_text(strip=True) if summary_node else \"\"\n",
    "\n",
    "        date_node = a.find_next(class_=\"date\")\n",
    "        date = date_node.get_text(strip=True) if date_node else \"\"\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"summary\": summary,\n",
    "            \"date\": date\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "# -----------------------------\n",
    "# 기사 본문 가져오기 (선택)\n",
    "# -----------------------------\n",
    "def get_article_body(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        content_div = soup.find(\"div\", class_=\"article-view-content\")\n",
    "        if content_div:\n",
    "            return content_div.get_text(\"\\n\", strip=True)\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] 본문 요청 실패: {url} → {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# -----------------------------\n",
    "# 전체 뉴스 수집\n",
    "# -----------------------------\n",
    "def crawl_news(pages=3, get_body=False, delay=0.5):\n",
    "    all_articles = []\n",
    "    for page in range(1, pages+1):\n",
    "        html = get_list_html(page)\n",
    "        if not html:\n",
    "            continue\n",
    "        articles = parse_list(html)\n",
    "\n",
    "        if get_body:\n",
    "            for a in articles:\n",
    "                a[\"content\"] = get_article_body(a[\"url\"])\n",
    "                time.sleep(delay)\n",
    "\n",
    "        all_articles.extend(articles)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "# -----------------------------\n",
    "# JSON 저장\n",
    "# -----------------------------\n",
    "def save_json(data, filename=\"aitimes_news.json\"):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    filepath = f\"results/{filename}\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[OK] 저장 완료: {filepath} ({len(data)}건)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 메인 실행\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pages_to_crawl = 5\n",
    "    collect_body = False  # True면 본문도 수집\n",
    "\n",
    "    articles = crawl_news(pages=pages_to_crawl, get_body=collect_body)\n",
    "    save_json(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  20%|█████████████▌                                                      | 1/5 [00:01<00:06,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed: https://www.aitimes.com/ajaxArticlePaging.php | 404 Client Error: Not Found for url: https://www.aitimes.com/ajaxArticlePaging.php?page=1&list_per_page=20&view_type=smBlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  40%|███████████████████████████▏                                        | 2/5 [00:02<00:04,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed: https://www.aitimes.com/ajaxArticlePaging.php | 404 Client Error: Not Found for url: https://www.aitimes.com/ajaxArticlePaging.php?page=2&list_per_page=20&view_type=smBlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  60%|████████████████████████████████████████▊                           | 3/5 [00:03<00:02,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed: https://www.aitimes.com/ajaxArticlePaging.php | 404 Client Error: Not Found for url: https://www.aitimes.com/ajaxArticlePaging.php?page=3&list_per_page=20&view_type=smBlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages:  80%|██████████████████████████████████████████████████████▍             | 4/5 [00:04<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed: https://www.aitimes.com/ajaxArticlePaging.php | 404 Client Error: Not Found for url: https://www.aitimes.com/ajaxArticlePaging.php?page=4&list_per_page=20&view_type=smBlock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling pages: 100%|████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Request failed: https://www.aitimes.com/ajaxArticlePaging.php | 404 Client Error: Not Found for url: https://www.aitimes.com/ajaxArticlePaging.php?page=5&list_per_page=20&view_type=smBlock\n",
      "[INFO] Saved 0 records to aitimes_articles.json\n",
      "{\n",
      "  \"total_articles\": 0,\n",
      "  \"date_min\": null,\n",
      "  \"date_max\": null,\n",
      "  \"by_day\": {}\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from dateutil import parser as dateparser\n",
    "from datetime import datetime\n",
    "\n",
    "BASE_URL = \"https://www.aitimes.com\"\n",
    "PAGING_URL = f\"{BASE_URL}/ajaxArticlePaging.php\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/123.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "TIMEOUT = 15\n",
    "REQUEST_DELAY = 0.8\n",
    "\n",
    "\n",
    "def fetch(url: str, params: Optional[Dict] = None) -> Optional[str]:\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, params=params, timeout=TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "        return resp.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[WARN] Request failed: {url} | {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_date(text: Optional[str]) -> Optional[str]:\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        dt = dateparser.parse(text)\n",
    "        if dt:\n",
    "            return dt.isoformat()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for sep in [\"-\", \".\", \"/\"]:\n",
    "        parts = text.replace(\"년\", sep).replace(\"월\", sep).replace(\"일\", \"\").split(sep)\n",
    "        nums = [p.strip() for p in parts if p.strip().isdigit()]\n",
    "        if len(nums) >= 3:\n",
    "            try:\n",
    "                y, m, d = int(nums[0]), int(nums[1]), int(nums[2])\n",
    "                return datetime(y, m, d).isoformat()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_list(html: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ajaxArticlePaging 응답 HTML에서 기사 목록 파싱.\n",
    "    실제 클래스/구조는 Network → Response → Elements에서 확인 후 선택자 조정.\n",
    "    \"\"\"\n",
    "    # lxml이 없다면 html.parser로 자동 폴백하는 방식 권장\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 예시: 카드 래퍼 선택자 (실제 구조에 맞게 바꾸세요)\n",
    "    items = soup.select(\".list-block-item, .article-list .item, li.type2, .news-list .item\")\n",
    "    if not items:\n",
    "        # 대체 선택자 시도\n",
    "        items = soup.select(\"li, article\")\n",
    "\n",
    "    results = []\n",
    "    for it in items:\n",
    "        # 제목/URL\n",
    "        a = it.select_one(\"a\")\n",
    "        title = a.get_text(strip=True) if a else None\n",
    "        href = a.get(\"href\") if a else None\n",
    "        url = None\n",
    "        if href:\n",
    "            url = href if href.startswith(\"http\") else BASE_URL + href\n",
    "\n",
    "        # 요약\n",
    "        summary_el = it.select_one(\".lead, .summary, p.lead, .desc\")\n",
    "        summary = summary_el.get_text(strip=True) if summary_el else None\n",
    "\n",
    "        # 날짜(목록 상 노출 시)\n",
    "        date_el = it.select_one(\".date, time, .info .date\")\n",
    "        raw_date = date_el.get_text(strip=True) if date_el else None\n",
    "        date_iso = normalize_date(raw_date)\n",
    "\n",
    "        if title and url:\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"summary\": summary,\n",
    "                \"date\": date_iso,\n",
    "                \"raw_date\": raw_date,\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_article(html: str) -> Dict:\n",
    "    \"\"\"\n",
    "    상세 페이지에서 본문/제목/날짜/기자 파싱.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 제목\n",
    "    title_el = soup.select_one(\"#articleTitle, .article-title, h1.title, h1\")\n",
    "    title_detail = title_el.get_text(strip=True) if title_el else None\n",
    "\n",
    "    # 본문\n",
    "    body = soup.select_one(\"#articleBody, #article-view-content-div, .article-body, .content, .news-body\")\n",
    "    content = None\n",
    "    if body:\n",
    "        for tag in body([\"script\", \"style\"]):\n",
    "            tag.decompose()\n",
    "        content = body.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    # 날짜\n",
    "    date_el = soup.select_one(\".date, .news-date, time, .info .date\")\n",
    "    raw_date_detail = date_el.get_text(strip=True) if date_el else None\n",
    "    date_detail = normalize_date(raw_date_detail)\n",
    "\n",
    "    # 기자\n",
    "    author_el = soup.select_one(\".author, .reporter, .byline .name\")\n",
    "    author = author_el.get_text(strip=True) if author_el else None\n",
    "\n",
    "    return {\n",
    "        \"title_detail\": title_detail,\n",
    "        \"content\": content,\n",
    "        \"date_detail\": date_detail,\n",
    "        \"raw_date_detail\": raw_date_detail,\n",
    "        \"author\": author,\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_list_pages(start_page: int = 1, end_page: int = 5,\n",
    "                     list_per_page: int = 20,\n",
    "                     view_type: str = \"smBlock\",\n",
    "                     extra_params: Optional[Dict] = None,\n",
    "                     collect_detail: bool = False) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ajaxArticlePaging.php?page=N 형태로 여러 페이지 수집.\n",
    "    Network 탭에서 발견한 파라미터(total, list_per_page, view_type, ixno 등 반영).\n",
    "    \"\"\"\n",
    "    data: List[Dict] = []\n",
    "    extra_params = extra_params or {}\n",
    "\n",
    "    for page in tqdm(range(start_page, end_page + 1), desc=\"Crawling pages\"):\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"list_per_page\": list_per_page,\n",
    "            \"view_type\": view_type,\n",
    "            # \"total\": 1439,  # 참고용(필수 아닐 수 있음)\n",
    "            # \"ixno\": 0,      # 네트워크 탭에서 보이면 반영\n",
    "        }\n",
    "        params.update(extra_params)\n",
    "\n",
    "        html = fetch(PAGING_URL, params=params)\n",
    "        if not html:\n",
    "            continue\n",
    "\n",
    "        items = parse_list(html)\n",
    "        if not items:\n",
    "            print(f\"[INFO] No items found on page {page}, stopping.\")\n",
    "            break\n",
    "\n",
    "        if collect_detail:\n",
    "            for it in items:\n",
    "                url = it.get(\"url\")\n",
    "                if not url:\n",
    "                    continue\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "                art_html = fetch(url)\n",
    "                if not art_html:\n",
    "                    continue\n",
    "                detail = parse_article(art_html)\n",
    "                it.update(detail)\n",
    "\n",
    "        data.extend(items)\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_json(data: List[Dict], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[INFO] Saved {len(data)} records to {path}\")\n",
    "\n",
    "\n",
    "def make_stats(data: List[Dict]) -> Dict:\n",
    "    def pick_date(rec):\n",
    "        return rec.get(\"date_detail\") or rec.get(\"date\")\n",
    "\n",
    "    dates = [pick_date(d) for d in data if pick_date(d)]\n",
    "    by_day: Dict[str, int] = {}\n",
    "    for iso in dates:\n",
    "        day = iso[:10]\n",
    "        by_day[day] = by_day.get(day, 0) + 1\n",
    "\n",
    "    date_min = min(dates) if dates else None\n",
    "    date_max = max(dates) if dates else None\n",
    "\n",
    "    return {\n",
    "        \"total_articles\": len(data),\n",
    "        \"date_min\": date_min,\n",
    "        \"date_max\": date_max,\n",
    "        \"by_day\": by_day\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 섹션/카테고리 필터가 있다면 Network 탭에서 파라미터 확인 후 반영\n",
    "    extra_params = {\n",
    "        # 예시: \"sc_section_code\": \"S1N1\"\n",
    "    }\n",
    "\n",
    "    # 목록만 수집\n",
    "    data = crawl_list_pages(start_page=1, end_page=5,\n",
    "                            list_per_page=20,\n",
    "                            view_type=\"smBlock\",\n",
    "                            extra_params=extra_params,\n",
    "                            collect_detail=False)\n",
    "\n",
    "    # 상세 본문까지 수집하려면 collect_detail=True로\n",
    "    # data = crawl_list_pages(start_page=1, end_page=3, list_per_page=20,\n",
    "    #                         view_type=\"smBlock\", extra_params=extra_params,\n",
    "    #                         collect_detail=True)\n",
    "\n",
    "    save_json(data, \"aitimes_articles.json\")\n",
    "    stats = make_stats(data)\n",
    "    print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
