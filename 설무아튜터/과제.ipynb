{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1463f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lizzy\\ìŠ¤íŒŒë¥´íƒ€_íŒŒì´ì¬\\ì„¤ë¬´ì•„íŠœí„°\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\lizzy\\ìŠ¤íŒŒë¥´íƒ€_íŒŒì´ì¬\\ì„¤ë¬´ì•„íŠœí„°\")\n",
    "print(os.getcwd())  # ì˜ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5406ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ 1í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\n",
      "ğŸ“„ 2í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\n",
      "âœ… aitimes_ai_news2.json ì €ì¥ ì™„ë£Œ (42ê°œ ê¸°ì‚¬)\n",
      "\n",
      "ğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\n",
      "date\n",
      "               2\n",
      "10-27 18:00    1\n",
      "10-27 16:40    1\n",
      "10-27 13:05    1\n",
      "10-24 17:00    1\n",
      "10-24 15:52    1\n",
      "10-23 16:54    1\n",
      "10-22 12:44    1\n",
      "10-29 15:45    1\n",
      "10-02 14:09    1\n",
      "10-01 06:00    1\n",
      "09-26 10:05    1\n",
      "09-30 18:11    1\n",
      "09-16 18:50    1\n",
      "09-15 16:50    1\n",
      "09-15 16:14    1\n",
      "09-18 18:30    1\n",
      "09-13 14:50    1\n",
      "09-13 10:20    1\n",
      "09-11 15:40    1\n",
      "09-10 16:05    1\n",
      "09-09 17:00    1\n",
      "09-09 12:27    1\n",
      "09-08 16:30    1\n",
      "09-08 16:05    1\n",
      "09-05 16:00    1\n",
      "09-04 14:50    1\n",
      "09-03 16:15    1\n",
      "09-01 14:50    1\n",
      "08-29 17:05    1\n",
      "08-29 16:35    1\n",
      "08-29 12:20    1\n",
      "08-28 11:41    1\n",
      "08-25 10:50    1\n",
      "08-24 16:25    1\n",
      "08-22 16:58    1\n",
      "08-22 16:00    1\n",
      "08-20 07:35    1\n",
      "08-19 16:30    1\n",
      "08-17 16:45    1\n",
      "08-15 07:26    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“° ìµœì‹  ê¸°ì‚¬:\n",
      "                                              title         date reporter\n",
      "0                ê³¼ê¸°ë¶€, APEC í•œ-ë¯¸ ì •ìƒíšŒë‹´ì„œ AIê¸°ìˆ  í˜‘ë ¥ MOU ì²´ê²°  10-29 15:45   ì¥ì„¸ë¯¼ ê¸°ì\n",
      "1  APEC CEO ì„œë°‹ì— ê¸€ë¡œë²Œ ê¸°ì—…ì¸ 1700ëª… ì°¸ì—¬...\"K-ì‚°ì—… ë¯¸ë˜ ì²­ì‚¬ì§„ ì œì‹œ\"  10-27 18:00   ì¥ì„¸ë¯¼ ê¸°ì\n",
      "2         ê³¼ê¸°ë¶€, 'AI ë…¸ì½”ë“œ í•´ì»¤í†¤' ê°œìµœ...\"20ì„¸ ì´ìƒ ëˆ„êµ¬ë‚˜ ì°¸ì—¬ ê°€ëŠ¥\"  10-27 16:40   ì¥ì„¸ë¯¼ ê¸°ì\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = \"https://www.aitimes.com\"\n",
    "\n",
    "def get_article_list(page: int = 1, section_code: str = \"S1N1\") -> list:\n",
    "    \"\"\"\n",
    "    AIíƒ€ì„ì¦ˆ ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.aitimes.com/news/articleList.html\"\n",
    "    params = {\n",
    "        \"page\": str(page),\n",
    "        \"sc_section_code\": section_code,\n",
    "        \"view_type\": \"sm\"\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    # ê¸°ì‚¬ ë¸”ë¡ ì„ íƒ\n",
    "    for div in soup.select(\"div.altlist-webzine-content\"):\n",
    "        title_tag = div.select_one(\"h2.altlist-subject a\")\n",
    "        info_items = div.select(\"div.altlist-info-item\")\n",
    "\n",
    "        if not title_tag or len(info_items) < 2:\n",
    "            continue\n",
    "\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        relative_url = title_tag[\"href\"]\n",
    "        url = urljoin(BASE_URL, relative_url)   # âœ… ì ˆëŒ€ê²½ë¡œ ë³€í™˜\n",
    "        reporter = info_items[0].get_text(strip=True)\n",
    "        date = info_items[1].get_text(strip=True)\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"reporter\": reporter,\n",
    "            \"date\": date\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "    \n",
    "def get_article_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    ê°œë³„ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # âœ… id=\"article-view-content-div\" ì•ˆì˜ ëª¨ë“  <p> ì„ íƒ\n",
    "        content_div = soup.find(\"article\", id=\"article-view-content-div\")\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "        \n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        content = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        return content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def crawl_ai_news(pages: int = 3):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ AI ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\"ğŸ“„ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        articles = get_article_list(page)\n",
    "\n",
    "        for article in articles:\n",
    "            article[\"content\"] = get_article_content(article[\"url\"])\n",
    "            all_articles.append(article)\n",
    "        sleep(0.1)  # í˜ì´ì§€ ë‹¨ìœ„ë¡œë§Œ ì‚´ì§ ì‰¬ê¸°\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "\n",
    "def save_to_json(data: list, filename: str = \"aitimes_ai_news2.json\"):\n",
    "    \"\"\"\n",
    "    ê²°ê³¼ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… {filename} ì €ì¥ ì™„ë£Œ ({len(data)}ê°œ ê¸°ì‚¬)\")\n",
    "\n",
    "\n",
    "def summarize_articles(data: list):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ë“± ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\")\n",
    "    print(df[\"date\"].value_counts())\n",
    "\n",
    "    print(\"\\nğŸ“° ìµœì‹  ê¸°ì‚¬:\")\n",
    "    print(df.head(3)[[\"title\", \"date\", \"reporter\"]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_ai_news(pages=2)  # ì›í•˜ëŠ” í˜ì´ì§€ ìˆ˜ ì¡°ì •\n",
    "    save_to_json(data)\n",
    "    summarize_articles(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
