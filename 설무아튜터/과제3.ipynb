{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "def get_article_list(page: int = 1, section_code: str = \"S1N1\") -> list:\n",
    "    \"\"\"\n",
    "    AIíƒ€ì„ì¦ˆ ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.aitimes.com/news/articleList.html\"\n",
    "    params = {\n",
    "        \"page\": str(page),\n",
    "        \"sc_section_code\": section_code,  # AI ê´€ë ¨ ì„¹ì…˜ ì½”ë“œ\n",
    "        \"view_type\": \"sm\"\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = []\n",
    "\n",
    "    # ê¸°ì‚¬ ë¸”ë¡ ì„ íƒ\n",
    "    for div in soup.select(\"div.altlist-webzine-content\"):\n",
    "        title_tag = div.select_one(\"h2.altlist-subject a\")\n",
    "        info_items = div.select(\"div.altlist-info-item\")\n",
    "\n",
    "        if not title_tag or len(info_items) < 2:\n",
    "            continue\n",
    "\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        url = title_tag[\"href\"]\n",
    "        reporter = info_items[0].get_text(strip=True)\n",
    "        date = info_items[1].get_text(strip=True)\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"reporter\": reporter,\n",
    "            \"date\": date\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_article_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    ê°œë³„ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      'Chrome/141.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        content_div = soup.select_one(\"div.article-body\")  # ë³¸ë¬¸ ë¶€ë¶„\n",
    "        if not content_div:\n",
    "            content_div = soup.select_one(\"div#article-view-content-div\")\n",
    "\n",
    "        content = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"\"\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def crawl_ai_news(pages: int = 3):\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ AI ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        print(f\"ğŸ“„ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        articles = get_article_list(page)\n",
    "\n",
    "        for article in articles:\n",
    "            article[\"content\"] = get_article_content(article[\"url\"])\n",
    "            all_articles.append(article)\n",
    "            sleep(1)  # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "\n",
    "def save_to_json(data: list, filename: str = \"aitimes_ai_news.json\"):\n",
    "    \"\"\"\n",
    "    ê²°ê³¼ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"âœ… {filename} ì €ì¥ ì™„ë£Œ ({len(data)}ê°œ ê¸°ì‚¬)\")\n",
    "\n",
    "\n",
    "def summarize_articles(data: list):\n",
    "    \"\"\"\n",
    "    ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ ë“± ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nğŸ“Š ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜:\")\n",
    "    print(df[\"date\"].value_counts())\n",
    "\n",
    "    print(\"\\nğŸ“° ìµœì‹  ê¸°ì‚¬:\")\n",
    "    print(df.head(3)[[\"title\", \"date\", \"reporter\"]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl_ai_news(pages=2)  # ì›í•˜ëŠ” í˜ì´ì§€ ìˆ˜ ì¡°ì •\n",
    "    save_to_json(data)\n",
    "    summarize_articles(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
